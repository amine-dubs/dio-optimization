\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{5}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \textbf  {Cross-Domain DIO Framework:} Overview showing DIO's application across both Medical (Breast Cancer, 30D) and Vision (CIFAR-10, 2048D) domains, with nested optimization structure. Medical: 96.88\% accuracy (XGBoost, Rank \#1), 10 features (67\% reduction). Vision: 81.91\% accuracy (XGBoost, Rank \#3, worse than defaults), 598 features (70.8\% reduction). Demonstrates 68× dimensional scale-up with success in low-D (30) but failure in high-D (2048) due to insufficient budget. }}{6}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:schema1_crossdomain}{{1}{6}{\textbf {Cross-Domain DIO Framework:} Overview showing DIO's application across both Medical (Breast Cancer, 30D) and Vision (CIFAR-10, 2048D) domains, with nested optimization structure. Medical: 96.88\% accuracy (XGBoost, Rank \#1), 10 features (67\% reduction). Vision: 81.91\% accuracy (XGBoost, Rank \#3, worse than defaults), 598 features (70.8\% reduction). Demonstrates 68× dimensional scale-up with success in low-D (30) but failure in high-D (2048) due to insufficient budget}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Methodology}{6}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Dholes-Inspired Optimization (DIO) Algorithm}{6}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}DIO Pseudocode}{8}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{lst:dio_pseudocode}{{1}{8}{Dhole-Inspired Optimization (DIO) Algorithm}{lstlisting.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}{\ignorespaces Dhole-Inspired Optimization (DIO) Algorithm}}{8}{lstlisting.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Algorithm Validation}{9}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  extbf{DIO Algorithm Flowchart:} Complete algorithmic flow showing initialization, fitness evaluation, three hunting strategies (chase alpha, scavenge, random movement), position updates, and convergence criteria. Source: El Romeh, Snášel, Mirjalili (2025) [1]. This flowchart illustrates the core mechanism adapted in our Python implementation for machine learning optimization. }}{10}{figure.caption.3}\protected@file@percent }
\newlabel{fig:dio_flowchart}{{2}{10}{extbf{DIO Algorithm Flowchart:} Complete algorithmic flow showing initialization, fitness evaluation, three hunting strategies (chase alpha, scavenge, random movement), position updates, and convergence criteria. Source: El Romeh, Snášel, Mirjalili (2025) [1]. This flowchart illustrates the core mechanism adapted in our Python implementation for machine learning optimization}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Benchmark Comparison with Other Metaheuristics}{11}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  extbf{DIO Performance on Engineering Benchmark:} Comparison of DIO with state-of-the-art metaheuristic algorithms (Genetic Algorithm, Particle Swarm Optimization, Differential Evolution, Grey Wolf Optimizer, Whale Optimization Algorithm, and others) on the Pressure Vessel Design Problem. DIO demonstrates competitive performance with best/mean/worst cost values comparable to or better than established methods. Source: El Romeh, Snášel, Mirjalili (2025) [1]. This benchmark validation supports our selection of DIO for machine learning hyperparameter optimization tasks. }}{11}{figure.caption.4}\protected@file@percent }
\newlabel{fig:dio_comparison}{{3}{11}{extbf{DIO Performance on Engineering Benchmark:} Comparison of DIO with state-of-the-art metaheuristic algorithms (Genetic Algorithm, Particle Swarm Optimization, Differential Evolution, Grey Wolf Optimizer, Whale Optimization Algorithm, and others) on the Pressure Vessel Design Problem. DIO demonstrates competitive performance with best/mean/worst cost values comparable to or better than established methods. Source: El Romeh, Snášel, Mirjalili (2025) [1]. This benchmark validation supports our selection of DIO for machine learning hyperparameter optimization tasks}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Random Forest Architecture}{11}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}XGBoost Architecture}{12}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Modeling DIO: From MATLAB to Python}{13}{subsection.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  \textbf  {DIO Python Implementation:} Core optimization loop showing population initialization, fitness evaluation, and iterative position updates using the three hunting strategies. This modular design enables seamless integration with scikit-learn and XGBoost classifiers. }}{14}{figure.caption.5}\protected@file@percent }
\newlabel{fig:dio_snippet}{{4}{14}{\textbf {DIO Python Implementation:} Core optimization loop showing population initialization, fitness evaluation, and iterative position updates using the three hunting strategies. This modular design enables seamless integration with scikit-learn and XGBoost classifiers}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Proposed Optimization Framework}{14}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Nested Optimization Structure}{14}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  \textbf  {Nested Optimization Architecture:} Hierarchical structure showing the Outer Loop (hyperparameter optimization) containing the Inner Loop (feature selection). Each outer iteration evaluates hyperparameters $\theta $ while the inner loop finds optimal features S* for that $\theta $. Medical RF (single-split): 50×200=10,000 evaluations (~60 min). Medical XGBoost: 50×200=10,000 evaluations (54 sec). Vision: 24×24=576 evaluations (215.98 min / 3.6 hrs). }}{16}{figure.caption.6}\protected@file@percent }
\newlabel{fig:schema4_nested}{{5}{16}{\textbf {Nested Optimization Architecture:} Hierarchical structure showing the Outer Loop (hyperparameter optimization) containing the Inner Loop (feature selection). Each outer iteration evaluates hyperparameters $\theta $ while the inner loop finds optimal features S* for that $\theta $. Medical RF (single-split): 50×200=10,000 evaluations (~60 min). Medical XGBoost: 50×200=10,000 evaluations (54 sec). Vision: 24×24=576 evaluations (215.98 min / 3.6 hrs)}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Fitness Function}{17}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  \textbf  {Fitness-Driven Optimization Mechanism:} The complete optimization process showing how fitness F = 0.99$\times $(1-Acc) + 0.01$\times $(Feat/Total) drives both nested loops. The outer loop tests hyperparameters $\theta $ while the inner loop (for each $\theta $) finds optimal features S*. Both minimize F simultaneously. Total evaluations = (Outer\_dholes $\times $ Outer\_iterations) $\times $ (Inner\_dholes $\times $ Inner\_iterations). This is the core technical schema explaining HOW the complete optimization mechanism works. }}{17}{figure.caption.7}\protected@file@percent }
\newlabel{fig:schema5_modularization}{{6}{17}{\textbf {Fitness-Driven Optimization Mechanism:} The complete optimization process showing how fitness F = 0.99$\times $(1-Acc) + 0.01$\times $(Feat/Total) drives both nested loops. The outer loop tests hyperparameters $\theta $ while the inner loop (for each $\theta $) finds optimal features S*. Both minimize F simultaneously. Total evaluations = (Outer\_dholes $\times $ Outer\_iterations) $\times $ (Inner\_dholes $\times $ Inner\_iterations). This is the core technical schema explaining HOW the complete optimization mechanism works}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  \textbf  {Feature Selection Objective Function:} Python implementation of the inner loop fitness function. For each feature subset (binary mask), the function trains a Random Forest with given hyperparameters and evaluates accuracy via cross-validation. Returns fitness = 0.99×(1-CV\_accuracy) + 0.01×(feature\_ratio) to be minimized. }}{18}{figure.caption.8}\protected@file@percent }
\newlabel{fig:feature_selection_obj}{{7}{18}{\textbf {Feature Selection Objective Function:} Python implementation of the inner loop fitness function. For each feature subset (binary mask), the function trains a Random Forest with given hyperparameters and evaluates accuracy via cross-validation. Returns fitness = 0.99×(1-CV\_accuracy) + 0.01×(feature\_ratio) to be minimized}{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  \textbf  {Hyperparameter Objective Function:} Outer loop fitness function that receives hyperparameters, launches inner DIO for feature selection, and returns the best fitness from optimizing features with those hyperparameters. This creates the nested optimization hierarchy. }}{19}{figure.caption.9}\protected@file@percent }
\newlabel{fig:hyperparameter_obj}{{8}{19}{\textbf {Hyperparameter Objective Function:} Outer loop fitness function that receives hyperparameters, launches inner DIO for feature selection, and returns the best fitness from optimizing features with those hyperparameters. This creates the nested optimization hierarchy}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Experimental Setup}{19}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Dataset Selection and Characteristics}{19}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}DIO Configuration}{20}{subsubsection.3.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  \textbf  {Outer Loop Execution and Results Retrieval:} Code showing how the outer DIO is launched with hyperparameter bounds, how it calls the inner loop for feature selection, and how final optimized hyperparameters and features are extracted after convergence. Demonstrates end-to-end optimization workflow. }}{21}{figure.caption.10}\protected@file@percent }
\newlabel{fig:outer_optimization}{{9}{21}{\textbf {Outer Loop Execution and Results Retrieval:} Code showing how the outer DIO is launched with hyperparameter bounds, how it calls the inner loop for feature selection, and how final optimized hyperparameters and features are extracted after convergence. Demonstrates end-to-end optimization workflow}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Validation Strategy}{21}{subsubsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Baseline Models}{22}{subsubsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.5}Statistical Analysis}{22}{subsubsection.3.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.6}Performance Metrics}{23}{subsubsection.3.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Optional Note: Hyper-Heuristics}{23}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results and Discussion}{23}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Overall Model Performance}{23}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Model Performance Summary over 30 Runs (Top 5 and DIO)}}{23}{table.caption.11}\protected@file@percent }
\newlabel{tab:model_summary}{{1}{23}{Model Performance Summary over 30 Runs (Top 5 and DIO)}{table.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Statistical Significance}{24}{subsection.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Wilcoxon Signed-Rank Test p-values (DIO-Optimized RF vs. Other Models)}}{24}{table.caption.12}\protected@file@percent }
\newlabel{tab:wilcoxon}{{2}{24}{Wilcoxon Signed-Rank Test p-values (DIO-Optimized RF vs. Other Models)}{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Visual Analysis}{24}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Six-panel comparison of all 10 models across 30 runs for single-split optimization approach.}}{25}{figure.caption.13}\protected@file@percent }
\newlabel{fig:main_viz}{{10}{25}{Six-panel comparison of all 10 models across 30 runs for single-split optimization approach}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Pareto-Optimal Solution}{25}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Feature Selection Analysis}{25}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Detailed Performance Comparison}{26}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Optimization Overfitting: A Critical Insight}{26}{subsection.4.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \textbf  {Single Run Optimization Results:} DIO-Optimized RF achieved 99\% accuracy (rank \#1) on the single optimization split. However, this impressive single-run performance masked severe overfitting—when validated across 30 different data splits, the same configuration dropped to 94.37\% (rank \#6). This stark degradation (from \#1 to \#6) demonstrates that hyperparameters can memorize quirks of a specific train/test partition rather than learning generalizable patterns.}}{27}{figure.caption.14}\protected@file@percent }
\newlabel{fig:single_run_overfitting}{{11}{27}{\textbf {Single Run Optimization Results:} DIO-Optimized RF achieved 99\% accuracy (rank \#1) on the single optimization split. However, this impressive single-run performance masked severe overfitting—when validated across 30 different data splits, the same configuration dropped to 94.37\% (rank \#6). This stark degradation (from \#1 to \#6) demonstrates that hyperparameters can memorize quirks of a specific train/test partition rather than learning generalizable patterns}{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  \textbf  {Algorithm-Dependent Optimization Overfitting Discovery:} Comparison of three approaches showing our key research contribution. RF Single-Split (5/10 outer, 10/20 inner) achieved 99\% during optimization but only 94.37\% ± 1.82\% validation (rank \#6) - severe overfitting. RF-CV (same config + 5-fold CV) fixed this: 96.55\% ± 1.51\% (rank \#1) but took 7.9 hours. XGBoost (5/10 outer, 10/20 inner) achieved best results 96.88\% ± 1.10\% (rank \#1) in only 54 seconds without CV, proving gradient boosting's built-in regularization prevents meta-overfitting. KEY FINDING: Algorithm choice determines whether cross-validation is necessary during optimization. }}{28}{figure.caption.15}\protected@file@percent }
\newlabel{fig:schema2_overfitting}{{12}{28}{\textbf {Algorithm-Dependent Optimization Overfitting Discovery:} Comparison of three approaches showing our key research contribution. RF Single-Split (5/10 outer, 10/20 inner) achieved 99\% during optimization but only 94.37\% ± 1.82\% validation (rank \#6) - severe overfitting. RF-CV (same config + 5-fold CV) fixed this: 96.55\% ± 1.51\% (rank \#1) but took 7.9 hours. XGBoost (5/10 outer, 10/20 inner) achieved best results 96.88\% ± 1.10\% (rank \#1) in only 54 seconds without CV, proving gradient boosting's built-in regularization prevents meta-overfitting. KEY FINDING: Algorithm choice determines whether cross-validation is necessary during optimization}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.7.1}The Phenomenon}{28}{subsubsection.4.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.7.2}Why This Matters}{28}{subsubsection.4.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.7.3}Recommended Approach}{29}{subsubsection.4.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}CV-Based Optimization: Validating the Solution}{29}{subsection.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.8.1}CV-Optimized Configuration}{29}{subsubsection.4.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces CV-based optimization convergence and model comparison visualization showing the optimization process across iterations.}}{30}{figure.caption.16}\protected@file@percent }
\newlabel{fig:cv_opt_viz}{{13}{30}{CV-based optimization convergence and model comparison visualization showing the optimization process across iterations}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.8.2}30-Run Statistical Validation}{30}{subsubsection.4.8.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces CV-Optimized Model Performance Summary (30 Runs)}}{30}{table.caption.17}\protected@file@percent }
\newlabel{tab:cv_results}{{3}{30}{CV-Optimized Model Performance Summary (30 Runs)}{table.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Six-panel comparison of CV-optimized model across 30 runs, showing improved stability and generalization.}}{31}{figure.caption.18}\protected@file@percent }
\newlabel{fig:cv_viz}{{14}{31}{Six-panel comparison of CV-optimized model across 30 runs, showing improved stability and generalization}{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Individual model performance trends across 30 independent runs for CV-optimized configuration. Each subplot shows accuracy (solid) and F1-score (dashed) trajectories, with red horizontal lines indicating mean accuracy.}}{31}{figure.caption.19}\protected@file@percent }
\newlabel{fig:cv_trends}{{15}{31}{Individual model performance trends across 30 independent runs for CV-optimized configuration. Each subplot shows accuracy (solid) and F1-score (dashed) trajectories, with red horizontal lines indicating mean accuracy}{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces ROC curves for CV-optimized model showing excellent discrimination capability with AUC near 1.0, demonstrating strong classification performance on both classes.}}{32}{figure.caption.20}\protected@file@percent }
\newlabel{fig:cv_roc}{{16}{32}{ROC curves for CV-optimized model showing excellent discrimination capability with AUC near 1.0, demonstrating strong classification performance on both classes}{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.8.3}Statistical Significance Analysis}{32}{subsubsection.4.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A Remaining Question}{33}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.8.4}Comparison: Single-Split vs. CV-Based}{33}{subsubsection.4.8.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Single-Split vs. CV-Based Optimization Comparison}}{33}{table.caption.22}\protected@file@percent }
\newlabel{tab:comparison}{{4}{33}{Single-Split vs. CV-Based Optimization Comparison}{table.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.8.5}Key Insights}{33}{subsubsection.4.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}Robustness and Generalization}{34}{subsection.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10}XGBoost Optimization: Exploring Gradient Boosting}{34}{subsection.4.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.10.1}XGBoost-Optimized Configuration}{34}{subsubsection.4.10.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  \textbf  {XGBoost Hyperparameter Search Space (Medical Domain):} Configuration showing the 5-dimensional search space for breast cancer classification: n\_estimators [10-200], max\_depth [1-20], learning\_rate [0.01-0.3], subsample [0.5-1.0], colsample\_bytree [0.5-1.0]. DIO simultaneously optimizes these parameters with feature selection in the nested framework. }}{35}{figure.caption.23}\protected@file@percent }
\newlabel{fig:xgb_search_space_cancer}{{17}{35}{\textbf {XGBoost Hyperparameter Search Space (Medical Domain):} Configuration showing the 5-dimensional search space for breast cancer classification: n\_estimators [10-200], max\_depth [1-20], learning\_rate [0.01-0.3], subsample [0.5-1.0], colsample\_bytree [0.5-1.0]. DIO simultaneously optimizes these parameters with feature selection in the nested framework}{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces XGBoost optimization convergence visualization showing fitness evolution and final model performance across the nested DIO optimization process.}}{36}{figure.caption.24}\protected@file@percent }
\newlabel{fig:xgb_opt_viz}{{18}{36}{XGBoost optimization convergence visualization showing fitness evolution and final model performance across the nested DIO optimization process}{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.10.2}30-Run Statistical Validation}{36}{subsubsection.4.10.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces XGBoost-Optimized Model Performance Summary (30 Runs)}}{36}{table.caption.25}\protected@file@percent }
\newlabel{tab:xgb_results}{{5}{36}{XGBoost-Optimized Model Performance Summary (30 Runs)}{table.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Six-panel comparison of XGBoost-optimized model across 30 runs, showing superior performance and stability.}}{37}{figure.caption.26}\protected@file@percent }
\newlabel{fig:xgb_viz}{{19}{37}{Six-panel comparison of XGBoost-optimized model across 30 runs, showing superior performance and stability}{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.10.3}Statistical Significance Analysis}{37}{subsubsection.4.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.10.4}Comparison: XGBoost vs. Random Forest Optimization}{38}{subsubsection.4.10.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces XGBoost vs. Random Forest DIO Optimization Comparison}}{38}{table.caption.27}\protected@file@percent }
\newlabel{tab:xgb_rf_comparison}{{6}{38}{XGBoost vs. Random Forest DIO Optimization Comparison}{table.caption.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.10.5}Clinical Deployment Recommendation}{38}{subsubsection.4.10.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11}Robustness and Generalization}{39}{subsection.4.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12}Clinical Deployment Recommendations}{39}{subsection.4.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13}Comparison with Hyper-Heuristic Approach}{40}{subsection.4.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14}Limitations}{40}{subsection.4.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.15}Future Work}{41}{subsection.4.15}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Extension to Image Classification: CIFAR-10 Deep Learning Features}{42}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Motivation for Dataset Extension}{42}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Dataset and Feature Extraction}{43}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Model Selection: Comparison Phase}{43}{subsection.5.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces CIFAR-10 Model Comparison Results (Full Dataset)}}{44}{table.caption.28}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces CIFAR-10 Sample Images: Representative examples from the 10 classes (automobile, frog, ship, deer, dog, bird, airplane, cat) used in the image classification experiments with ResNet50 features.}}{44}{figure.caption.29}\protected@file@percent }
\newlabel{fig:cifar10_samples}{{20}{44}{CIFAR-10 Sample Images: Representative examples from the 10 classes (automobile, frog, ship, deer, dog, bird, airplane, cat) used in the image classification experiments with ResNet50 features}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Test Accuracy Comparison: XGBoost (85.0\%), Gradient Boosting (82.0\%), and Random Forest (83.0\%) on CIFAR-10 with ResNet50 features.}}{44}{figure.caption.30}\protected@file@percent }
\newlabel{fig:test_accuracy}{{21}{44}{Test Accuracy Comparison: XGBoost (85.0\%), Gradient Boosting (82.0\%), and Random Forest (83.0\%) on CIFAR-10 with ResNet50 features}{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}DIO Optimization Configuration}{45}{subsection.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces  \textbf  {XGBoost Hyperparameter Search Space (Vision Domain):} Configuration for CIFAR-10 image classification showing the 3-dimensional search space: n\_estimators [30-100], max\_depth [3-10], learning\_rate [0.01-0.3]. Reduced search space compared to medical domain due to computational constraints of 2048-D feature space. }}{46}{figure.caption.31}\protected@file@percent }
\newlabel{fig:xgb_search_space_images}{{22}{46}{\textbf {XGBoost Hyperparameter Search Space (Vision Domain):} Configuration for CIFAR-10 image classification showing the 3-dimensional search space: n\_estimators [30-100], max\_depth [3-10], learning\_rate [0.01-0.3]. Reduced search space compared to medical domain due to computational constraints of 2048-D feature space}{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Optimization Results}{46}{subsection.5.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces DIO-Optimized XGBoost for CIFAR-10 (Subset: 2K Train, 500 Test)}}{47}{table.caption.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Visualization and Interpretation}{48}{subsection.5.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces DIO-Optimized XGBoost Performance on CIFAR-10 Subset: (Left) Test accuracy comparison showing 2.2\% improvement over baseline (83.0\% vs 80.8\%). (Right) Feature count reduction from 2,048 to 598 features (70.8\% reduction) while achieving superior accuracy.}}{48}{figure.caption.33}\protected@file@percent }
\newlabel{fig:cifar10_results}{{23}{48}{DIO-Optimized XGBoost Performance on CIFAR-10 Subset: (Left) Test accuracy comparison showing 2.2\% improvement over baseline (83.0\% vs 80.8\%). (Right) Feature count reduction from 2,048 to 598 features (70.8\% reduction) while achieving superior accuracy}{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Statistical Validation: 30-Run Comparison}{49}{subsection.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.7.1}Experimental Setup}{49}{subsubsection.5.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.7.2}Results: Optimization Overfitting Revealed}{49}{subsubsection.5.7.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Statistical Comparison of CIFAR-10 Models (30 Runs)}}{49}{table.caption.34}\protected@file@percent }
\newlabel{tab:cifar_stats}{{9}{49}{Statistical Comparison of CIFAR-10 Models (30 Runs)}{table.caption.34}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Wilcoxon Signed-Rank Test: DIO-XGBoost vs. Baselines (CIFAR-10)}}{49}{table.caption.35}\protected@file@percent }
\newlabel{tab:cifar_wilcoxon}{{10}{49}{Wilcoxon Signed-Rank Test: DIO-XGBoost vs. Baselines (CIFAR-10)}{table.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Statistical Comparison of CIFAR-10 Models Across 30 Runs: (Left) Accuracy distribution boxplots showing XGBoost Default (All features) as the clear winner. (Right) Mean accuracy with standard deviation, revealing that DIO-optimized configuration ranks 3rd, significantly underperforming default XGBoost (p$<$0.0001).}}{50}{figure.caption.36}\protected@file@percent }
\newlabel{fig:cifar_stats}{{24}{50}{Statistical Comparison of CIFAR-10 Models Across 30 Runs: (Left) Accuracy distribution boxplots showing XGBoost Default (All features) as the clear winner. (Right) Mean accuracy with standard deviation, revealing that DIO-optimized configuration ranks 3rd, significantly underperforming default XGBoost (p$<$0.0001)}{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.7.3}Critical Findings: Optimization Overfitting in High Dimensions}{51}{subsubsection.5.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces  \textbf  {CIFAR-10 Optimization Failure: Budget-Induced Overfitting.} Statistical analysis showing DIO-XGBoost (81.91\% $\pm $ 1.38\%, Rank \#3) significantly underperforms default XGBoost (83.27\% $\pm $ 1.25\%, Rank \#1) with p=7.15$\times $10\textsuperscript  {-5} (***). Root cause: Insufficient budget (576 evaluations) for 2048-D space---need 10,000-50,000 evaluations (17-87$\times $ more). Three failure mechanisms: (1) Budget OFF BY 17-87$\times $, (2) Optimization overfitting despite XGBoost regularization, (3) Dimensionality curse (2048-D vs 30-D medical). Critical lesson: Even robust algorithms fail with inadequate computational resources---budget must scale with dimensionality\textsuperscript  {2}. Honest negative result demonstrates research integrity and validates importance of proper resource allocation. }}{51}{figure.caption.37}\protected@file@percent }
\newlabel{fig:schema7_cifar_failure}{{25}{51}{\textbf {CIFAR-10 Optimization Failure: Budget-Induced Overfitting.} Statistical analysis showing DIO-XGBoost (81.91\% $\pm $ 1.38\%, Rank \#3) significantly underperforms default XGBoost (83.27\% $\pm $ 1.25\%, Rank \#1) with p=7.15$\times $10\textsuperscript {-5} (***). Root cause: Insufficient budget (576 evaluations) for 2048-D space---need 10,000-50,000 evaluations (17-87$\times $ more). Three failure mechanisms: (1) Budget OFF BY 17-87$\times $, (2) Optimization overfitting despite XGBoost regularization, (3) Dimensionality curse (2048-D vs 30-D medical). Critical lesson: Even robust algorithms fail with inadequate computational resources---budget must scale with dimensionality\textsuperscript {2}. Honest negative result demonstrates research integrity and validates importance of proper resource allocation}{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}Comparative Analysis: Medical vs. Image Data}{53}{subsection.5.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces DIO Performance Across Domains (Statistical Validation)}}{53}{table.caption.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9}Real-World Applications for Vision Tasks}{54}{subsection.5.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.10}Limitations and Future Work}{54}{subsection.5.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.11}Summary of Image Classification Extension}{55}{subsection.5.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{56}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Summary of Contributions}{56}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Key Findings Across Domains}{58}{subsection.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces  \textbf  {Three-Approach Research Evolution:} Timeline showing progression from RF Single-Split (discovered optimization overfitting, $\sim $60 min, 94.37\% $\pm $ 1.82\%, rank \#6) $\rightarrow $ RF-CV (fixed overfitting with 5-fold CV, 7.9 hrs, 96.55\% $\pm $ 1.51\%, rank \#1) $\rightarrow $ XGBoost (best solution with built-in regularization, 54 sec, 96.88\% $\pm $ 1.10\%, rank \#1). Key insight: XGBoost achieves top accuracy 526$\times $ faster than RF-CV, proving algorithm choice determines whether cross-validation is necessary during optimization. All use same inner loop config (10 dholes $\times $ 20 iterations), XGBoost uses simpler outer (5 dholes $\times $ 10 iterations). }}{58}{figure.caption.39}\protected@file@percent }
\newlabel{fig:schema6_evolution}{{26}{58}{\textbf {Three-Approach Research Evolution:} Timeline showing progression from RF Single-Split (discovered optimization overfitting, $\sim $60 min, 94.37\% $\pm $ 1.82\%, rank \#6) $\rightarrow $ RF-CV (fixed overfitting with 5-fold CV, 7.9 hrs, 96.55\% $\pm $ 1.51\%, rank \#1) $\rightarrow $ XGBoost (best solution with built-in regularization, 54 sec, 96.88\% $\pm $ 1.10\%, rank \#1). Key insight: XGBoost achieves top accuracy 526$\times $ faster than RF-CV, proving algorithm choice determines whether cross-validation is necessary during optimization. All use same inner loop config (10 dholes $\times $ 20 iterations), XGBoost uses simpler outer (5 dholes $\times $ 10 iterations)}{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces  \textbf  {Cross-Domain Validation Results:} Quantitative comparison showing Medical SUCCESS (30D, 96.88\% ± 1.10\%, rank \#1, p=0.0047, 10 features, 54 sec) vs CIFAR-10 FAILURE (2048D, 81.91\% ± 1.38\%, rank \#3 worse than 83.27\% defaults, p<0.0001, 598 features, 215.98 min/3.6 hrs). Critical insight: 30-D with 10,000 evals → SUCCESS; 2048-D with 576 evals → FAILURE. Optimization budget must scale with dimensionality², requiring 17-87× more evaluations for high-dimensional problems. Validates that computational investment determines cross-domain success. }}{60}{figure.caption.40}\protected@file@percent }
\newlabel{fig:schema3_comparison}{{27}{60}{\textbf {Cross-Domain Validation Results:} Quantitative comparison showing Medical SUCCESS (30D, 96.88\% ± 1.10\%, rank \#1, p=0.0047, 10 features, 54 sec) vs CIFAR-10 FAILURE (2048D, 81.91\% ± 1.38\%, rank \#3 worse than 83.27\% defaults, p<0.0001, 598 features, 215.98 min/3.6 hrs). Critical insight: 30-D with 10,000 evals → SUCCESS; 2048-D with 576 evals → FAILURE. Optimization budget must scale with dimensionality², requiring 17-87× more evaluations for high-dimensional problems. Validates that computational investment determines cross-domain success}{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Practical Impact Across Domains}{60}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Broader Implications}{61}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Final Remarks}{62}{subsection.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Acknowledgments}{63}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{63}{section*.42}\protected@file@percent }
\gdef \@abspage@last{64}
